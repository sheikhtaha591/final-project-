{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "nt-dQsyuOk7I",
        "outputId": "c7bc5e2e-b194-4a92-c2c6-d299cd088a2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "'return' outside function (ipython-input-1337947970.py, line 100)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-1337947970.py\"\u001b[0;36m, line \u001b[0;32m100\u001b[0m\n\u001b[0;31m    im=generate_image(en); tr=translate(en,\"en\",t); return tr,im\u001b[0m\n\u001b[0m                                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'return' outside function\n"
          ]
        }
      ],
      "source": [
        "# ============================\n",
        "# 🌍 Sheikh Taha's Multimodal Translator Chatbot\n",
        "# Final Internship Project (One-Page Version)\n",
        "# ============================\n",
        "\n",
        "!pip install -q gradio transformers sentencepiece accelerate safetensors timm gTTS diffusers pillow\n",
        "\n",
        "import os, time, torch\n",
        "import gradio as gr\n",
        "from PIL import Image, ImageDraw\n",
        "from gtts import gTTS\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "\n",
        "# -------- Settings --------\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "HUGGINGFACE_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\", None)\n",
        "PAIR2MODEL = {\"en>ur\":\"Helsinki-NLP/opus-mt-en-ur\",\"ur>en\":\"Helsinki-NLP/opus-mt-ur-en\",\n",
        "              \"en>fr\":\"Helsinki-NLP/opus-mt-en-fr\",\"fr>en\":\"Helsinki-NLP/opus-mt-fr-en\",\n",
        "              \"en>es\":\"Helsinki-NLP/opus-mt-en-es\",\"es>en\":\"Helsinki-NLP/opus-mt-es-en\"}\n",
        "LANGS = {\"en\":\"English\",\"ur\":\"Urdu\",\"fr\":\"French\",\"es\":\"Spanish\"}\n",
        "GTTS_LANG = {\"en\":\"en\",\"ur\":\"ur\",\"fr\":\"fr\",\"es\":\"es\"}\n",
        "_translators, _asr, _captioner, _sd = {}, None, None, None\n",
        "\n",
        "# -------- Translation --------\n",
        "def load_translator(pair):\n",
        "    if pair not in _translators:\n",
        "        tok = AutoTokenizer.from_pretrained(PAIR2MODEL[pair])\n",
        "        mdl = AutoModelForSeq2SeqLM.from_pretrained(PAIR2MODEL[pair]).to(DEVICE)\n",
        "        _translators[pair] = (tok, mdl)\n",
        "    return _translators[pair]\n",
        "\n",
        "def translate(text, src, tgt):\n",
        "    if src == tgt: return text\n",
        "    if src != \"en\":\n",
        "        tok, mdl = load_translator(f\"{src}>en\")\n",
        "        out = mdl.generate(**tok(text, return_tensors=\"pt\", truncation=True).to(DEVICE), max_new_tokens=256)\n",
        "        text = tok.decode(out[0], skip_special_tokens=True)\n",
        "    if tgt != \"en\":\n",
        "        tok, mdl = load_translator(f\"en>{tgt}\")\n",
        "        out = mdl.generate(**tok(text, return_tensors=\"pt\", truncation=True).to(DEVICE), max_new_tokens=256)\n",
        "        text = tok.decode(out[0], skip_special_tokens=True)\n",
        "    return text\n",
        "\n",
        "# -------- Text-to-Speech --------\n",
        "def text_to_speech(text, lang):\n",
        "    fn = f\"tts_{int(time.time()*1000)}.mp3\"\n",
        "    gTTS(text=text, lang=GTTS_LANG.get(lang,\"en\")).save(fn)\n",
        "    return fn\n",
        "\n",
        "# -------- ASR --------\n",
        "def asr(audio_path):\n",
        "    global _asr\n",
        "    if _asr is None:\n",
        "        _asr = pipeline(\"automatic-speech-recognition\",\"openai/whisper-base\",device=0 if DEVICE==\"cuda\" else -1)\n",
        "    return _asr(audio_path)[\"text\"]\n",
        "\n",
        "# -------- Image Captioning --------\n",
        "def caption_image(img):\n",
        "    global _captioner\n",
        "    if _captioner is None:\n",
        "        _captioner = pipeline(\"image-to-text\",\"nlpconnect/vit-gpt2-image-captioning\",device=0 if DEVICE==\"cuda\" else -1)\n",
        "    return _captioner(img)[0][\"generated_text\"]\n",
        "\n",
        "# -------- Image Generation --------\n",
        "def fallback_poster(prompt,w=512,h=512):\n",
        "    img = Image.new(\"RGB\",(w,h),(40,40,70)); d=ImageDraw.Draw(img)\n",
        "    d.text((20,20),\"\\n\".join([prompt[i:i+25] for i in range(0,len(prompt),25)]),fill=(255,255,255)); return img\n",
        "\n",
        "def generate_image(prompt):\n",
        "    global _sd\n",
        "    if _sd is None:\n",
        "        try:\n",
        "            from diffusers import StableDiffusionPipeline\n",
        "            _sd = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\",\n",
        "                    use_auth_token=HUGGINGFACE_TOKEN,torch_dtype=torch.float16 if DEVICE==\"cuda\" else torch.float32).to(DEVICE)\n",
        "        except Exception: return fallback_poster(prompt)\n",
        "    return _sd(prompt, num_inference_steps=20).images[0]\n",
        "\n",
        "# -------- Gradio UI --------\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\"# 🌐 Multimodal Translator: Text ↔ Voice ↔ Image\")\n",
        "\n",
        "    with gr.Tab(\"📝 Text ➜ 🎤 Voice\"):\n",
        "        src=gr.Dropdown(list(LANGS.keys()),value=\"en\",label=\"Source\")\n",
        "        tgt=gr.Dropdown(list(LANGS.keys()),value=\"ur\",label=\"Target\")\n",
        "        inp=gr.Textbox(label=\"Enter text\"); out_txt=gr.Textbox(label=\"Translated\"); out_aud=gr.Audio(type=\"filepath\")\n",
        "        gr.Button(\"Translate & Speak\").click(lambda x,s,t:(translate(x,s,t),text_to_speech(translate(x,s,t),t)),\n",
        "                                            [inp,src,tgt],[out_txt,out_aud])\n",
        "\n",
        "    with gr.Tab(\"🖼️ Image ➜ 🎤 Voice\"):\n",
        "        tgt2=gr.Dropdown(list(LANGS.keys()),value=\"en\",label=\"Voice Language\"); img=gr.Image(type=\"pil\")\n",
        "        cap_out=gr.Textbox(label=\"Caption\"); aud2=gr.Audio(type=\"filepath\")\n",
        "        gr.Button(\"Caption & Speak\").click(lambda im,t:(translate(caption_image(im),'en',t),text_to_speech(translate(caption_image(im),'en',t),t)),\n",
        "                                           [img,tgt2],[cap_out,aud2])\n",
        "\n",
        "    with gr.Tab(\"🎤 Voice ➜ 🖼️ Image\"):\n",
        "        tgt3=gr.Dropdown(list(LANGS.keys()),value=\"en\",label=\"Prompt Lang\")\n",
        "        aud=gr.Audio(sources=[\"microphone\"],type=\"filepath\"); txt=gr.Textbox(); out_img=gr.Image(type=\"pil\")\n",
        "        def run_voice(a,t): speech=asr(a); en=translate(speech,t,\"en\") if t!=\"en\" else speech;\n",
        "        im=generate_image(en); tr=translate(en,\"en\",t); return tr,im\n",
        "        gr.Button(\"Generate Image\").click(run_voice,[aud,tgt3],[txt,out_img])\n",
        "\n",
        "demo.launch(share=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# 🌍 Sheikh Taha's Multimodal Translator Chatbot\n",
        "# Final Internship Project (One-Page Fixed Version)\n",
        "# ============================\n",
        "\n",
        "!pip install -q gradio transformers sentencepiece accelerate safetensors timm gTTS diffusers pillow\n",
        "\n",
        "import os, time, torch\n",
        "import gradio as gr\n",
        "from PIL import Image, ImageDraw\n",
        "from gtts import gTTS\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "\n",
        "# -------- Settings --------\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "HUGGINGFACE_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\", None)\n",
        "PAIR2MODEL = {\"en>ur\":\"Helsinki-NLP/opus-mt-en-ur\",\"ur>en\":\"Helsinki-NLP/opus-mt-ur-en\",\n",
        "              \"en>fr\":\"Helsinki-NLP/opus-mt-en-fr\",\"fr>en\":\"Helsinki-NLP/opus-mt-fr-en\",\n",
        "              \"en>es\":\"Helsinki-NLP/opus-mt-en-es\",\"es>en\":\"Helsinki-NLP/opus-mt-es-en\"}\n",
        "LANGS = {\"en\":\"English\",\"ur\":\"Urdu\",\"fr\":\"French\",\"es\":\"Spanish\"}\n",
        "GTTS_LANG = {\"en\":\"en\",\"ur\":\"ur\",\"fr\":\"fr\",\"es\":\"es\"}\n",
        "_translators, _asr, _captioner, _sd = {}, None, None, None\n",
        "\n",
        "# -------- Translation --------\n",
        "def load_translator(pair):\n",
        "    if pair not in _translators:\n",
        "        tok = AutoTokenizer.from_pretrained(PAIR2MODEL[pair])\n",
        "        mdl = AutoModelForSeq2SeqLM.from_pretrained(PAIR2MODEL[pair]).to(DEVICE)\n",
        "        _translators[pair] = (tok, mdl)\n",
        "    return _translators[pair]\n",
        "\n",
        "def translate(text, src, tgt):\n",
        "    if src == tgt: return text\n",
        "    if src != \"en\":\n",
        "        tok, mdl = load_translator(f\"{src}>en\")\n",
        "        out = mdl.generate(**tok(text, return_tensors=\"pt\", truncation=True).to(DEVICE), max_new_tokens=256)\n",
        "        text = tok.decode(out[0], skip_special_tokens=True)\n",
        "    if tgt != \"en\":\n",
        "        tok, mdl = load_translator(f\"en>{tgt}\")\n",
        "        out = mdl.generate(**tok(text, return_tensors=\"pt\", truncation=True).to(DEVICE), max_new_tokens=256)\n",
        "        text = tok.decode(out[0], skip_special_tokens=True)\n",
        "    return text\n",
        "\n",
        "# -------- Text-to-Speech --------\n",
        "def text_to_speech(text, lang):\n",
        "    fn = f\"tts_{int(time.time()*1000)}.mp3\"\n",
        "    gTTS(text=text, lang=GTTS_LANG.get(lang,\"en\")).save(fn)\n",
        "    return fn\n",
        "\n",
        "# -------- ASR --------\n",
        "def asr(audio_path):\n",
        "    global _asr\n",
        "    if _asr is None:\n",
        "        _asr = pipeline(\"automatic-speech-recognition\",\"openai/whisper-base\",device=0 if DEVICE==\"cuda\" else -1)\n",
        "    return _asr(audio_path)[\"text\"]\n",
        "\n",
        "# -------- Image Captioning --------\n",
        "def caption_image(img):\n",
        "    global _captioner\n",
        "    if _captioner is None:\n",
        "        _captioner = pipeline(\"image-to-text\",\"nlpconnect/vit-gpt2-image-captioning\",device=0 if DEVICE==\"cuda\" else -1)\n",
        "    return _captioner(img)[0][\"generated_text\"]\n",
        "\n",
        "# -------- Image Generation --------\n",
        "def fallback_poster(prompt,w=512,h=512):\n",
        "    img = Image.new(\"RGB\",(w,h),(40,40,70)); d=ImageDraw.Draw(img)\n",
        "    d.text((20,20),\"\\n\".join([prompt[i:i+25] for i in range(0,len(prompt),25)]),fill=(255,255,255)); return img\n",
        "\n",
        "def generate_image(prompt):\n",
        "    global _sd\n",
        "    if _sd is None:\n",
        "        try:\n",
        "            from diffusers import StableDiffusionPipeline\n",
        "            _sd = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\",\n",
        "                    use_auth_token=HUGGINGFACE_TOKEN,torch_dtype=torch.float16 if DEVICE==\"cuda\" else torch.float32).to(DEVICE)\n",
        "        except Exception: return fallback_poster(prompt)\n",
        "    return _sd(prompt, num_inference_steps=20).images[0]\n",
        "\n",
        "# -------- Gradio UI --------\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\"# 🌐 Multimodal Translator: Text ↔ Voice ↔ Image\")\n",
        "\n",
        "    with gr.Tab(\"📝 Text ➜ 🎤 Voice\"):\n",
        "        src=gr.Dropdown(list(LANGS.keys()),value=\"en\",label=\"Source\")\n",
        "        tgt=gr.Dropdown(list(LANGS.keys()),value=\"ur\",label=\"Target\")\n",
        "        inp=gr.Textbox(label=\"Enter text\"); out_txt=gr.Textbox(label=\"Translated\"); out_aud=gr.Audio(type=\"filepath\")\n",
        "        def run_text(x,s,t): tr=translate(x,s,t); aud=text_to_speech(tr,t); return tr,aud\n",
        "        gr.Button(\"Translate & Speak\").click(run_text,[inp,src,tgt],[out_txt,out_aud])\n",
        "\n",
        "    with gr.Tab(\"🖼️ Image ➜ 🎤 Voice\"):\n",
        "        tgt2=gr.Dropdown(list(LANGS.keys()),value=\"en\",label=\"Voice Language\"); img=gr.Image(type=\"pil\")\n",
        "        cap_out=gr.Textbox(label=\"Caption\"); aud2=gr.Audio(type=\"filepath\")\n",
        "        def run_img(im,t): cap=caption_image(im); tr=translate(cap,'en',t); mp3=text_to_speech(tr,t); return tr,mp3\n",
        "        gr.Button(\"Caption & Speak\").click(run_img,[img,tgt2],[cap_out,aud2])\n",
        "\n",
        "    with gr.Tab(\"🎤 Voice ➜ 🖼️ Image\"):\n",
        "        tgt3=gr.Dropdown(list(LANGS.keys()),value=\"en\",label=\"Prompt Lang\")\n",
        "        aud=gr.Audio(sources=[\"microphone\"],type=\"filepath\"); txt=gr.Textbox(); out_img=gr.Image(type=\"pil\")\n",
        "        def run_voice(a,t):\n",
        "            speech=asr(a); en=translate(speech,t,\"en\") if t!=\"en\" else speech\n",
        "            im=generate_image(en); tr=translate(en,\"en\",t); return tr,im\n",
        "        gr.Button(\"Generate Image\").click(run_voice,[aud,tgt3],[txt,out_img])\n",
        "\n",
        "demo.launch(share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 614
        },
        "id": "eJEl7PZgRjwH",
        "outputId": "0ef103d7-4b20-4af3-9853-196a20dcfe1c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://3650ed34f799ff3f0f.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://3650ed34f799ff3f0f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    }
  ]
}